SparkLoaderPostgreMongoDB
Carregamento de dados com spark do postgres para o mongoDB



# üöÄ Projeto - ETL com PySpark, PostgreSQL e MongoDB

Este projeto realiza a ingest√£o de dados a partir de arquivos CSV e JSON, transformando-os com PySpark e salvando em um 
banco de dados PostgreSQL automaticamente. Ele inclui cria√ß√£o do banco de dados, tabelas, ajuste de tipos e inser√ß√£o dos 
dados.

---

## üì¶ Requisitos

- Python 3.8+
- Java (JDK 8 ou superior)
- PostgreSQL instalado e rodando
- Apache Spark
- Driver JDBC do PostgreSQL (`.jar`)

---

## üîß Instala√ß√£o
Clone o reposit√≥rio:

git clone https://github.com/lemysonlemos/SparkLoaderPostgreMongoDB.git

Crie e ative um ambiente virtual (opcional, mas recomendado):
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate no Windows

Rodar o projeto via docker:
docker-compose up --build

Rodar o projeto via jupyter notebook:
No arquivo etl.ipynb, consta todo o etl, verifique os caminhos e rode.


IMPORTANTE

Os arquivos, clientes.json, vendas.csv e produtos.csv precisam ser baixados no 
link: https://drive.google.com/drive/u/0/folders/1U79YSFP08K8PoRRN_onQI3ELaXvGTRyq ou
fa√ßa coloque as suas tabelas, pois s√£o arquivos grandes e n√£o pode subir no github

