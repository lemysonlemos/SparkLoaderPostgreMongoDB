{
 "cells": [
  {
   "cell_type": "code",
   "id": "0e0ac7e8-5528-4c5c-aa45-15fc80909c77",
   "metadata": {},
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import csv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, to_date, dayofweek, row_number\n",
    "from pyspark.sql.window import Window"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdcf97d6-8dee-4074-bd08-13a3e4231d82",
   "metadata": {},
   "source": [
    "# inicializa√ß√£o do etl, criando o banco, tabelas, spark, processando os arquivos e executando\n",
    "\n",
    "class DataImporter:\n",
    "    def __init__(self, db_config, arquivos, driver_jar):\n",
    "        self.db_name = db_config[\"dbname\"]\n",
    "        self.db_user = db_config[\"user\"]\n",
    "        self.db_password = db_config[\"password\"]\n",
    "        self.db_host = db_config[\"host\"]\n",
    "        self.db_port = db_config[\"port\"]\n",
    "        self.arquivos = arquivos\n",
    "        self.driver_jar = driver_jar\n",
    "        self.spark = None\n",
    "\n",
    "    def criar_banco(self):\n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                dbname=\"postgres\",\n",
    "                user=self.db_user,\n",
    "                password=self.db_password,\n",
    "                host=self.db_host\n",
    "            )\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(f\"SELECT 1 FROM pg_database WHERE datname = '{self.db_name}'\")\n",
    "            if not cur.fetchone():\n",
    "                cur.execute(f\"CREATE DATABASE {self.db_name}\")\n",
    "                print(f\"üü¢ Banco '{self.db_name}' criado.\")\n",
    "            else:\n",
    "                print(f\"üü° Banco '{self.db_name}' j√° existe.\")\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao criar banco: {e}\")\n",
    "\n",
    "    def iniciar_spark(self):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Importar Arquivos para PostgreSQL\") \\\n",
    "            .config(\"spark.jars\", self.driver_jar) \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def mapear_tipo_spark_para_postgres(self, spark_type):\n",
    "        mapping = {\n",
    "            \"string\": \"TEXT\",\n",
    "            \"int\": \"INTEGER\",\n",
    "            \"bigint\": \"BIGINT\",\n",
    "            \"double\": \"DOUBLE PRECISION\",\n",
    "            \"float\": \"REAL\",\n",
    "            \"boolean\": \"BOOLEAN\",\n",
    "            \"timestamp\": \"TIMESTAMP\",\n",
    "            \"date\": \"DATE\"\n",
    "        }\n",
    "        return mapping.get(spark_type.lower(), \"TEXT\")\n",
    "\n",
    "    def criar_tabela(self, tabela, df):\n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                user=self.db_user,\n",
    "                password=self.db_password,\n",
    "                host=self.db_host\n",
    "            )\n",
    "            cur = conn.cursor()\n",
    "            schema = df.schema\n",
    "            colunas_sql = []\n",
    "            for field in schema.fields:\n",
    "                tipo_sql = self.mapear_tipo_spark_para_postgres(field.dataType.simpleString())\n",
    "                colunas_sql.append(f'\"{field.name}\" {tipo_sql}')\n",
    "            sql_create = f'CREATE TABLE IF NOT EXISTS \"{tabela}\" ({\", \".join(colunas_sql)});'\n",
    "            cur.execute(sql_create)\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(f\"üü¢ Tabela '{tabela}' criada com sucesso.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao criar tabela '{tabela}': {e}\")\n",
    "\n",
    "    def salvar_no_postgres(self, df, tabela):\n",
    "        url = f\"jdbc:postgresql://{self.db_host}:{self.db_port}/{self.db_name}\"\n",
    "        props = {\n",
    "            \"user\": self.db_user,\n",
    "            \"password\": self.db_password,\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        df.write.jdbc(url=url, table=tabela, mode=\"append\", properties=props)\n",
    "        print(f\"‚úÖ Dados salvos na tabela '{tabela}'.\")\n",
    "\n",
    "    def ajustar_tipos(self, df, tabela):\n",
    "        if tabela == \"clientes\":\n",
    "            df = df.withColumn(\"COD_ID_CLIENTE\", col(\"COD_ID_CLIENTE\").cast(\"int\")) \\\n",
    "                   .withColumn(\"DAT_DATA_NASCIMENTO\", to_date(\"DAT_DATA_NASCIMENTO\", \"yyyy-MM-dd\"))\n",
    "        elif tabela == \"produtos\":\n",
    "            df = df.withColumn(\"COD_ID_PRODUTO\", col(\"COD_ID_PRODUTO\").cast(\"int\")) \\\n",
    "                   .withColumn(\"COD_ID_CATEGORIA_PRODUTO\", col(\"COD_ID_CATEGORIA_PRODUTO\").cast(\"int\")) \\\n",
    "                   .withColumn(\"COD_CODIGO_BARRAS\", col(\"COD_CODIGO_BARRAS\").cast(\"int\"))\n",
    "        elif tabela == \"vendas\":\n",
    "            df = df.withColumn(\"_c0\", col(\"_c0\").cast(\"int\")) \\\n",
    "                   .withColumn(\"COD_ID_LOJA\", col(\"COD_ID_LOJA\").cast(\"int\")) \\\n",
    "                   .withColumn(\"COD_ID_CLIENTE\", col(\"COD_ID_CLIENTE\").cast(\"int\")) \\\n",
    "                   .withColumn(\"NUM_ANOMESDIA\", col(\"NUM_ANOMESDIA\").cast(\"int\")) \\\n",
    "                   .withColumn(\"COD_ID_VENDA_UNICO\", col(\"COD_ID_VENDA_UNICO\").cast(\"int\")) \\\n",
    "                   .withColumn(\"COD_ID_PRODUTO\", col(\"COD_ID_PRODUTO\").cast(\"int\")) \\\n",
    "                   .withColumn(\"VAL_VALOR_SEM_DESC\", round(col(\"VAL_VALOR_SEM_DESC\"), 2).cast(\"decimal(18,2)\")) \\\n",
    "                   .withColumn(\"VAL_VALOR_DESCONTO\", round(col(\"VAL_VALOR_DESCONTO\"), 2).cast(\"decimal(18,2)\")) \\\n",
    "                   .withColumn(\"VAL_VALOR_COM_DESC\", round(col(\"VAL_VALOR_COM_DESC\"), 2).cast(\"decimal(18,2)\")) \\\n",
    "                   .withColumn(\"VAL_QUANTIDADE_KG\", round(col(\"VAL_QUANTIDADE_KG\"), 2).cast(\"decimal(18,2)\"))\n",
    "        return df\n",
    "\n",
    "    def processar_arquivo(self, path, tabela):\n",
    "        ext = os.path.splitext(path)[-1].lower()\n",
    "        if ext == \".csv\":\n",
    "            df = self.spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(path)\n",
    "        elif ext == \".json\":\n",
    "            df = self.spark.read.option(\"multiline\", \"true\").json(path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Tipo de arquivo n√£o suportado: {path}\")\n",
    "            return\n",
    "\n",
    "        # Ajustar tipos espec√≠ficos\n",
    "        df = self.ajustar_tipos(df, tabela)\n",
    "\n",
    "        # Otimizar particionamento\n",
    "        df = df.repartition(2)\n",
    "\n",
    "        self.criar_tabela(tabela, df)\n",
    "        self.salvar_no_postgres(df, tabela)\n",
    "\n",
    "    def executar(self):\n",
    "        self.criar_banco()\n",
    "        self.iniciar_spark()\n",
    "        for arquivo in self.arquivos:\n",
    "            print(f\"\\nüìÅ Processando: {arquivo['path']}\")\n",
    "            self.processar_arquivo(arquivo[\"path\"], arquivo[\"tabela\"])\n",
    "        self.spark.stop()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "852afda4-85aa-414b-8eb5-ca34c87fc788",
   "metadata": {},
   "source": [
    "# Configura√ß√µes e caminhos para execu√ß√£o\n",
    "db_config = {\n",
    "    \"dbname\": \"bancopostgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "arquivos = [\n",
    "    {\"path\": \"/vendas.csv\", \"tabela\": \"vendas\"},\n",
    "    {\"path\": \"/produtos.csv\", \"tabela\": \"produtos\"},\n",
    "    {\"path\": \"/clientes.json\", \"tabela\": \"clientes\"}\n",
    "]\n",
    "\n",
    "driver_jar=\"./postgresql-42.7.7.jar\"\n",
    "\n",
    "importador = DataImporter(db_config, arquivos, driver_jar)\n",
    "importador.executar()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b30f2a41-080b-4d2a-93a7-0f337dd13823",
   "metadata": {},
   "source": [
    "# Configura√ß√£o do banco\n",
    "db_config = {\n",
    "    \"dbname\": \"bancopostgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "# Cria diret√≥rio para salvar CSVs\n",
    "output_dir = \"./indicadores_sql_csv\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Queries com aspas duplas para nomes de colunas e tabelas\n",
    "# a. Produtos mais vendidos.\n",
    "# b. Clientes com mais compras.\n",
    "# c. Quantidade de vendas por dia.\n",
    "# d. Quantidade de produtos distintos vendidos por dia.\n",
    "# e. Produtos que concederam maior desconto.\n",
    "queries = {\n",
    "    \"produtos_mais_vendidos.csv\": \"\"\"\n",
    "        SELECT v.\"COD_ID_PRODUTO\", p.\"DES_PRODUTO\", SUM(CAST(v.\"VAL_QUANTIDADE_KG\" AS numeric)) AS total_vendido\n",
    "        FROM vendas v\n",
    "        LEFT JOIN produtos p ON v.\"COD_ID_PRODUTO\" = p.\"COD_ID_PRODUTO\"\n",
    "        GROUP BY v.\"COD_ID_PRODUTO\", p.\"DES_PRODUTO\"\n",
    "        ORDER BY total_vendido DESC;\n",
    "    \"\"\",\n",
    "    \"clientes_mais_compras.csv\": \"\"\"\n",
    "        SELECT v.\"COD_ID_CLIENTE\", c.\"NOM_NOME\", COUNT(DISTINCT v.\"COD_ID_VENDA_UNICO\") AS total_compras\n",
    "        FROM vendas v\n",
    "        LEFT JOIN clientes c ON v.\"COD_ID_CLIENTE\" = c.\"COD_ID_CLIENTE\"\n",
    "        GROUP BY v.\"COD_ID_CLIENTE\", c.\"NOM_NOME\"\n",
    "        ORDER BY total_compras DESC;\n",
    "    \"\"\",\n",
    "    \"vendas_por_dia.csv\": \"\"\"\n",
    "        SELECT \"NUM_ANOMESDIA\", COUNT(DISTINCT \"COD_ID_VENDA_UNICO\") AS qtde_vendas\n",
    "        FROM vendas\n",
    "        GROUP BY \"NUM_ANOMESDIA\"\n",
    "        ORDER BY \"NUM_ANOMESDIA\";\n",
    "    \"\"\",\n",
    "    \"produtos_distintos_por_dia.csv\": \"\"\"\n",
    "        SELECT \"NUM_ANOMESDIA\", COUNT(DISTINCT \"COD_ID_PRODUTO\") AS qtde_produtos_distintos\n",
    "        FROM vendas\n",
    "        GROUP BY \"NUM_ANOMESDIA\"\n",
    "        ORDER BY \"NUM_ANOMESDIA\";\n",
    "    \"\"\",\n",
    "    \"produtos_maior_desconto.csv\": \"\"\"\n",
    "        SELECT v.\"COD_ID_PRODUTO\", p.\"DES_PRODUTO\", SUM(CAST(v.\"VAL_VALOR_DESCONTO\" AS numeric)) AS total_desconto\n",
    "        FROM vendas v\n",
    "        LEFT JOIN produtos p ON v.\"COD_ID_PRODUTO\" = p.\"COD_ID_PRODUTO\"\n",
    "        GROUP BY v.\"COD_ID_PRODUTO\", p.\"DES_PRODUTO\"\n",
    "        ORDER BY total_desconto DESC;\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "def exportar_csv(nome_arquivo, query):\n",
    "    with psycopg2.connect(**db_config) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            colunas = [desc[0] for desc in cur.description]\n",
    "            resultados = cur.fetchall()\n",
    "\n",
    "    with open(os.path.join(output_dir, nome_arquivo), mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(colunas)\n",
    "        writer.writerows(resultados)\n",
    "\n",
    "    print(f\"‚úÖ Exportado: {nome_arquivo}\")\n",
    "\n",
    "for arquivo, query in queries.items():\n",
    "    exportar_csv(arquivo, query)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aaa61a28-f679-4cba-9f03-287e2cfd7cf6",
   "metadata": {},
   "source": [
    "# 1. Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PipelineUnificada\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.jars\", \"./postgresql-42.7.7.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Configura√ß√µes do banco\n",
    "db_config = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/bancopostgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# 3. Fun√ß√£o para ler tabelas\n",
    "def ler_tabela(tabela):\n",
    "    return spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", db_config[\"url\"]) \\\n",
    "        .option(\"dbtable\", tabela) \\\n",
    "        .option(\"user\", db_config[\"user\"]) \\\n",
    "        .option(\"password\", db_config[\"password\"]) \\\n",
    "        .option(\"driver\", db_config[\"driver\"]) \\\n",
    "        .load()\n",
    "\n",
    "# 4. Carregar tabelas\n",
    "df_vendas = ler_tabela(\"vendas\")\n",
    "df_clientes = ler_tabela(\"clientes\")\n",
    "df_produtos = ler_tabela(\"produtos\")\n",
    "\n",
    "# ==================== AJUSTE C√ìDIGO VENDA ====================\n",
    "tamanho_cod_loja = 3\n",
    "df_vendas = df_vendas.withColumn(\"COD_ID_LOJA_STR\", col(\"COD_ID_LOJA\").cast(\"string\")) \\\n",
    "    .withColumn(\"prefixo_cod_venda\", substring(col(\"COD_ID_VENDA_UNICO\"), 1, tamanho_cod_loja)) \\\n",
    "    .withColumn(\"sufixo_cod_venda\", substring(col(\"COD_ID_VENDA_UNICO\"), tamanho_cod_loja + 1, 100)) \\\n",
    "    .withColumn(\"COD_ID_VENDA_UNICO\",\n",
    "        when(col(\"prefixo_cod_venda\") != col(\"COD_ID_LOJA_STR\"),\n",
    "             concat(col(\"COD_ID_LOJA_STR\"), col(\"sufixo_cod_venda\")))\n",
    "        .otherwise(col(\"COD_ID_VENDA_UNICO\"))\n",
    "    ) \\\n",
    "    .drop(\"prefixo_cod_venda\", \"sufixo_cod_venda\", \"COD_ID_LOJA_STR\")\n",
    "\n",
    "# ==================== MELHOR DIA DA SEMANA - TOP 20 CLIENTES ====================\n",
    "# Converter NUM_ANOMESDIA (ex: 20230621) para data\n",
    "df_vendas = df_vendas.withColumn(\"DATA_VENDA\",\n",
    "    to_date(col(\"NUM_ANOMESDIA\").cast(\"string\"), \"yyyyMMdd\")\n",
    ")\n",
    "\n",
    "# Top 20 clientes com mais compras\n",
    "top_clientes = df_vendas.groupBy(\"COD_ID_CLIENTE\") \\\n",
    "    .agg(count(\"COD_ID_VENDA_UNICO\").alias(\"TOTAL_COMPRAS\")) \\\n",
    "    .orderBy(desc(\"TOTAL_COMPRAS\")) \\\n",
    "    .limit(20)\n",
    "\n",
    "# Filtrar vendas dos top clientes e extrair dia da semana\n",
    "df_top_vendas = df_vendas.join(top_clientes.select(\"COD_ID_CLIENTE\"), on=\"COD_ID_CLIENTE\", how=\"inner\") \\\n",
    "    .withColumn(\"DIA_SEMANA\", dayofweek(col(\"DATA_VENDA\")))\n",
    "\n",
    "# Contar vendas por cliente e dia da semana\n",
    "compras_por_dia = df_top_vendas.groupBy(\"COD_ID_CLIENTE\", \"DIA_SEMANA\") \\\n",
    "    .agg(count(\"COD_ID_VENDA_UNICO\").alias(\"QTDE_VENDAS\"))\n",
    "\n",
    "# Ranking do melhor dia\n",
    "window_cliente = Window.partitionBy(\"COD_ID_CLIENTE\").orderBy(desc(\"QTDE_VENDAS\"))\n",
    "melhor_dia_cliente = compras_por_dia.withColumn(\"rank\", row_number().over(window_cliente)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .drop(\"rank\")\n",
    "\n",
    "# Juntar com nomes\n",
    "resultado_melhor_dia = melhor_dia_cliente.join(df_clientes.select(\"COD_ID_CLIENTE\", \"NOM_NOME\"),\n",
    "                                               on=\"COD_ID_CLIENTE\", how=\"left\") \\\n",
    "    .orderBy(desc(\"QTDE_VENDAS\"))\n",
    "\n",
    "# ==================== VALIDA√á√ÉO CHAVES ESTRANGEIRAS ====================\n",
    "clientes_faltando = df_vendas.select(\"COD_ID_CLIENTE\").distinct() \\\n",
    "    .join(df_clientes.select(\"COD_ID_CLIENTE\").distinct(),\n",
    "          on=\"COD_ID_CLIENTE\", how=\"left_anti\") \\\n",
    "    .orderBy(\"COD_ID_CLIENTE\")\n",
    "\n",
    "produtos_faltando = df_vendas.select(\"COD_ID_PRODUTO\").distinct() \\\n",
    "    .join(df_produtos.select(\"COD_ID_PRODUTO\").distinct(),\n",
    "          on=\"COD_ID_PRODUTO\", how=\"left_anti\") \\\n",
    "    .orderBy(\"COD_ID_PRODUTO\")\n",
    "\n",
    "# ==================== SALVAR RESULTADOS ====================\n",
    "output_dir = \"./indicadores_csv\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "resultado_melhor_dia.write.mode(\"overwrite\").csv(os.path.join(output_dir, \"melhor_dia_semana_top20_clientes.csv\"), header=True)\n",
    "clientes_faltando.write.mode(\"overwrite\").csv(os.path.join(output_dir, \"clientes_faltando.csv\"), header=True)\n",
    "produtos_faltando.write.mode(\"overwrite\").csv(os.path.join(output_dir, \"produtos_faltando.csv\"), header=True)\n",
    "\n",
    "print(f\"‚úÖ Resultados exportados para: {output_dir}\")\n",
    "\n",
    "# ==================== FINALIZAR ====================\n",
    "spark.stop()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c123ac6-bc87-4534-9f61-e1933311b0b7",
   "metadata": {},
   "source": [
    "# --- script para fazer um replica do bancopostgres no mongoDB ---\n",
    "\n",
    "# URI de conex√£o para o MongoDB\n",
    "# 'localhost:27017' √© o endere√ßo padr√£o para uma inst√¢ncia local do MongoDB\n",
    "mongo_uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "# Nome do banco de dados no MongoDB onde os dados ser√£o escritos\n",
    "mongo_db = \"replica_bancomongo\"\n",
    "\n",
    "# URL de conex√£o para o PostgreSQL\n",
    "# 'localhost:5432' √© a porta padr√£o do PostgreSQL, 'bancopostgres' √© o nome do banco de dados\n",
    "pg_url = \"jdbc:postgresql://localhost:5432/bancopostgres\"\n",
    "\n",
    "# Credenciais de usu√°rio e senha para o PostgreSQL\n",
    "pg_user = \"postgres\"\n",
    "pg_password = \"postgres\"\n",
    "\n",
    "# Nome do driver JDBC para PostgreSQL\n",
    "pg_driver = \"org.postgresql.Driver\"\n",
    "\n",
    "# Caminho para o arquivo JAR do driver JDBC PostgreSQL.\n",
    "# O conector MongoDB Spark ser√° baixado automaticamente via 'spark.jars.packages'.\n",
    "postgresql_driver_jar = \"/postgresql-42.7.7.jar\"\n",
    "\n",
    "# --- Inicializa√ß√£o da SparkSession ---\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReplicaPostgresMongo\") \\\n",
    "    .config(\"spark.jars\", postgresql_driver_jar) \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "    .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"SparkSession inicializada com sucesso!\")\n",
    "\n",
    "# --- Fun√ß√£o Auxiliar para Replicar Tabelas ---\n",
    "def replicate_table(spark_session, table_name, mongo_db_name, mongo_collection_name):\n",
    "\n",
    "    print(f\"\\n--- Replicando Tabela: {table_name} ---\")\n",
    "    print(f\"Lendo dados da tabela '{table_name}' do PostgreSQL em: {pg_url}\")\n",
    "\n",
    "    try:\n",
    "        # L√™ os dados da tabela do PostgreSQL usando o formato JDBC.\n",
    "        df = spark_session.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", pg_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", pg_user) \\\n",
    "            .option(\"password\", pg_password) \\\n",
    "            .option(\"driver\", pg_driver) \\\n",
    "            .load()\n",
    "\n",
    "        print(f\"Dados da tabela '{table_name}' do PostgreSQL lidos com sucesso. Exemplo de esquema:\")\n",
    "        df.printSchema()\n",
    "\n",
    "        if table_name != \"vendas\":\n",
    "            print(f\"Exemplo de 5 linhas dos dados lidos da tabela '{table_name}':\")\n",
    "            df.show(5)\n",
    "        else:\n",
    "            print(f\"A tabela '{table_name}' pode ser grande. N√£o exibindo as primeiras 5 linhas para evitar problemas de mem√≥ria.\")\n",
    "\n",
    "\n",
    "        print(f\"Escrevendo dados no MongoDB na cole√ß√£o '{mongo_collection_name}' do banco de dados '{mongo_db_name}'...\")\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"database\", mongo_db_name) \\\n",
    "            .option(\"collection\", mongo_collection_name) \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"Dados da tabela '{table_name}' replicados para o MongoDB com sucesso na cole√ß√£o '{mongo_collection_name}'!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a replica√ß√£o da tabela '{table_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Chamadas da Fun√ß√£o de Replica√ß√£o para cada Tabela ---\n",
    "try:\n",
    "    # Replicar tabela 'clientes'\n",
    "    replicate_table(spark, \"clientes\", mongo_db, \"clientes\")\n",
    "\n",
    "    # Replicar tabela 'produtos'\n",
    "    replicate_table(spark, \"produtos\", mongo_db, \"produtos\")\n",
    "\n",
    "    # Replicar tabela 'vendas'\n",
    "    replicate_table(spark, \"vendas\", mongo_db, \"vendas\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nUm erro geral ocorreu durante a execu√ß√£o do script: {e}\")\n",
    "finally:\n",
    "    spark.stop()\n",
    "    print(\"\\nSparkSession encerrada.\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
